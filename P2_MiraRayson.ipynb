{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHANGE ALL COMMENTS THROUGHOUT CODE TO YOUR OWN WORDS (not the markdown though)\n",
    "\n",
    "Check out the notes in 'access_mnist_fashionmnist.ipynb' to write up description for this part.\n",
    "\n",
    "The following section takes you up to and including step 5 in project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to see if GPU support is available (ended up being unavailable and had to continue the project using CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU support is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU support is available.\")\n",
    "else:\n",
    "    print(\"GPU support is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the dataset\n",
    "download_path = r'C:\\\\Users\\\\mirar\\\\OneDrive\\\\Desktop\\\\WINTER_TERM\\\\Mining_Modeling\\\\P2\\\\MNIST'\n",
    "\n",
    "# Check if the directory exists, if not create it\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Create the training dataset object\n",
    "trainset = torchvision.datasets.MNIST(root=download_path, train=True, download=True, transform=transform)\n",
    "\n",
    "# Create the test dataset object\n",
    "testset = torchvision.datasets.MNIST(root=download_path, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 5)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(trainset)\n",
    "\n",
    "image, label = next(train_iter)\n",
    "\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting example image to ensure data has been loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 300  # This selects the image from the training data by index number\n",
    "\n",
    "image, label = trainset[index] # Get the image and its label\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(f\"Label: {classes[label]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining length of training and testing subsets, then subsetting the training data into training and validation datasets using a proportional split of 80% for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 12000, 10000)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset, valset = torch.utils.data.random_split(trainset, [0.80, 0.20])\n",
    "len(trainset), len(valset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training set: 4800\n",
      "Number of batches in the validation set: 1200\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of batches in the training set: {int(38400 / batch_size)}')\n",
    "print(f'Number of batches in the validation set: {int(9600 / batch_size)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining CNN architecture, ensuring formatting for greyscale (MNIST) imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Calculate the size of the flattened features after all convolutional and pooling layers\n",
    "        self._flattened_features = self._get_conv_output_size()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=self._flattened_features, out_features=1024)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def _get_conv_output_size(self):\n",
    "        # Create a dummy input to pass through the convolutional layers only\n",
    "        dummy_input = torch.zeros([1, 1, 28, 28])  # Use the provided input dimensions\n",
    "        dummy_input = self.conv1(dummy_input)\n",
    "        dummy_input = self.pool1(dummy_input)\n",
    "        dummy_input = self.conv2(dummy_input)\n",
    "        dummy_input = self.pool2(dummy_input)\n",
    "        dummy_input = self.conv3(dummy_input)\n",
    "        dummy_input = self.pool3(dummy_input)\n",
    "        return int(torch.flatten(dummy_input, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional and pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers with ReLU activations and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting loss function and optimization technique.\n",
    "\n",
    "See chatgpt answer for why these are suitable and why you didn't want to play around with them: \"\n",
    "For the MNIST dataset and grayscale imagery, the choices of loss function and optimization technique you've selected are quite suitable and commonly used:\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "nn.CrossEntropyLoss(): This is a good choice for multi-class classification tasks like the MNIST dataset. It combines the softmax activation function and the negative log-likelihood loss. It's particularly useful when dealing with classification problems where each sample can only belong to one class, which is the case for MNIST digits.\n",
    "Optimization Technique:\n",
    "\n",
    "optim.Adam(net.parameters(), lr=0.0001): Adam is a popular optimization algorithm that adapts the learning rate for each parameter. It's generally effective for a wide range of deep learning tasks and is often a good choice as a default optimizer. The learning rate you've chosen (0.0001) is commonly used for training neural networks and should work well for MNIST.\n",
    "Given the characteristics of the MNIST dataset and the task of classifying grayscale images of handwritten digits into their respective classes, the choices you've made are appropriate and should yield good results. It's always a good idea to start with these commonly used choices and then experiment with different options as needed, based on the specific characteristics of your model and dataset.\"\n",
    "\n",
    "DELETE THAT AFTERWARDS!!!!!!!!!!!!!!!!!!!!^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and running the training loop, then running training with a single epoch as a first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "  net.train(True)\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_accuracy = 0.0\n",
    "\n",
    "  for batch_index, (inputs, labels) in enumerate(trainset):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(inputs) # shape: [batch_size, 5]\n",
    "    correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
    "    running_accuracy += correct / batch_size\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    running_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_index % 500 == 499:  # print every 10 batches\n",
    "      avg_loss_across_batches = running_loss / 500\n",
    "      avg_acc_across_batches = (running_accuracy / 500) * 100\n",
    "      print('Batch {0}, Loss: {1:.3f}, Accuracy: {2:.1f}%'.format(batch_index+1,\n",
    "                                                          avg_loss_across_batches,\n",
    "                                                          avg_acc_across_batches))\n",
    "      running_loss = 0.0\n",
    "      running_accuracy = 0.0\n",
    "\n",
    "    \n",
    "  print(\"Input shape:\", inputs.shape)\n",
    "  print(\"Input head:\", inputs[:5])  # Print the first 5 elements\n",
    "  print(\"Output shape:\", outputs.shape)\n",
    "  print(\"Output head:\", outputs[:5])  # Print the first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch():\n",
    "    net.train(False)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(valset):\n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs) # shape: [batch_size, 5]\n",
    "            correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
    "            running_accuracy += correct / batch_size\n",
    "            loss = criterion(outputs, labels) # One number, the average batch loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    avg_loss_across_batches = running_loss / len(valset)\n",
    "    avg_acc_across_batches = (running_accuracy / len(valset)) * 100\n",
    "    \n",
    "    print('Val Loss: {0:.3f}, Val Accuracy: {1:.1f}%'.format(avg_loss_across_batches,\n",
    "                                                            avg_acc_across_batches))\n",
    "    print('***************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x9 and 2304x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[194], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     validate_one_epoch()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[192], line 11\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainset):\n\u001b[0;32m      9\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shape: [batch_size, 5]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m   correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(labels \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     13\u001b[0m   running_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m batch_size\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[190], line 47\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Fully connected layers with ReLU activations and dropout\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[0;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x9 and 2304x1024)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch_index in range(num_epochs):\n",
    "    print(f'Epoch: {epoch_index + 1}\\n')\n",
    "    \n",
    "    train_one_epoch()\n",
    "    validate_one_epoch()\n",
    "    \n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
